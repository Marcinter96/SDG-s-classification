{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\marco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries \n",
    "import nltk.data\n",
    "nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/french.pickle')\n",
    "import pandas as pd\n",
    "import os \n",
    "import sys \n",
    "#For images analysis\n",
    "import PIL\n",
    "from PIL import Image \n",
    "import pytesseract \n",
    "import pdf2image\n",
    "from pdf2image import convert_from_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scanextractor(path,file): \n",
    "    \"\"\"\n",
    "    Extract text from a scanned PDF and returns a dataframe where each sentences is a new row \n",
    "    of the returned dataframe.\n",
    "    INPUT: -path (string): path that contain the wanted document.\n",
    "           -name (string): name of the csv document to be used for predictions.\n",
    "    OUTPUT: - df(dataframe): dataframe containing all the sentences of the text\n",
    "    \"\"\"\n",
    "    PDF_file = (file)\n",
    "    ''' \n",
    "    Part #1 : Converting PDF to images \n",
    "    '''\n",
    "    # Store all the pages of the PDF in a variable \n",
    "    pages = convert_from_path(os.path.join(path,file), 500) \n",
    "    # Counter to store images of each page of PDF to image \n",
    "    image_counter = 1\n",
    "    # Iterate through all the pages stored above \n",
    "    for page in pages: \n",
    "    # Declaring filename for each page of PDF as JPG \n",
    "    # For each page, filename will be: \n",
    "    # PDF page 1 -> page_1.jpg \n",
    "    # .... \n",
    "    # PDF page n -> page_n.jpg \n",
    "        filename = \"page_\"+str(image_counter)+\".jpg\"\n",
    "        # Save the image of the page in system \n",
    "        page.save(filename, 'JPEG') \n",
    "        # Increment the counter to update filename \n",
    "        image_counter = image_counter + 1\n",
    "    ''' \n",
    "    Part #2 - Recognizing text from the images using OCR \n",
    "    '''\n",
    "    # Variable to get count of total number of pages \n",
    "    filelimit = image_counter-1\n",
    "    # Creating a text file to write the output \n",
    "    outfile = \"{}.txt\".format(PDF_file)\n",
    "    # Open the file in append mode so that \n",
    "    # All contents of all images are added to the same file \n",
    "    f = open(outfile, \"a\") \n",
    "    # Iterate from 1 to total number of pages \n",
    "    for i in range(1, filelimit + 1): \n",
    "    # Set filename to recognize text from \n",
    "    # Again, these files will be: \n",
    "    # page_1.jpg \n",
    "    # .... \n",
    "    # page_n.jpg \n",
    "        filename = \"page_\"+str(i)+\".jpg\"\n",
    "        # Recognize the text as string in image using pytesserct \n",
    "        text = str(((pytesseract.image_to_string(Image.open(filename))))) \n",
    "        # The recognized text is stored in variable text \n",
    "        # Any string processing may be applied on text \n",
    "        text = text.replace('-\\n', '')\n",
    "        # Finally, write the processed text to the file. \n",
    "        f.write(text) \n",
    "        os.remove(\"page_\"+str(i)+\".jpg\")\n",
    "    # Close the file after writing all the text. \n",
    "    f.close() \n",
    "    with codecs.open(\"{}.txt\".format(PDF_file),encoding=\"latin-1\") as f:\n",
    "        file_data = (f.read())\n",
    "        text= file_data.replace('\\n', '')\n",
    "        text = text.splitlines()\n",
    "        text = [el.replace('\\xa0',' ') for el in text]\n",
    "        text = [el.replace('\\x92',\"'\") for el in text]\n",
    "        text = [el.replace('\\x80',\" \") for el in text]            \n",
    "        filename = os.path.basename(file)\n",
    "        # Remove .pdf from the filename so we can use it as the name of the plot and PNG\n",
    "        csvname = filename.strip('.txt')\n",
    "        if text is None:\n",
    "            print(\"Error PDF cannot be read\")\n",
    "            sys.exit()\n",
    "        N = len(text)\n",
    "        text = [t for t in text if \".....\" not in t]\n",
    "        listToStr = ' '.join(map(str, text))\n",
    "        listToStr = listToStr.replace(\"’\", \"'\")\n",
    "        listToStr = listToStr.replace(\"-\",' ')\n",
    "        data = (tokenizer.tokenize(listToStr))\n",
    "        new_data = []\n",
    "        for i in range(int(len(data)/2)):\n",
    "            j = 2*i\n",
    "            new_data.append(data[j] + \" \" + data[j+1])\n",
    "        df = pd.DataFrame(new_data)\n",
    "        df = df.rename(columns={0:\"Kept sentences\"})\n",
    "        df.to_csv(\"Table_{}.csv\".format(csvname))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Step 1: Insert Path Name C:/Users/marco/Desktop/Desktop/OECD/Codes/2014/Afrique/\n",
      "Step 2: Insert File Name CA II.3.1.1 BENIN.pdf\n"
     ]
    }
   ],
   "source": [
    "path = input(\"Step 1: Insert Path Name :\")\n",
    "file = input(\"Step 2: Insert File Name :\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kept sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Annexe 6  Avis Indépendant Développement Durab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Elle prendra la forme d'une densification du r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ce projet d'extension et de renforcement des r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32Annexe 6  Avis Indépendant Développement Dur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elle prendra la forme d'une densification du r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Le classement risque souverain s'étend de RCI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>30S oxouuy  I          Q[BIOOS UOTPIPSWII}UI |...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>9p mod Jo sINUeNd  Sose}solap sap souonboyy s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>Elle prendra la forme d'une densification du r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>Ce projet d'extension et de renforcement des r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Kept sentences\n",
       "0    Annexe 6  Avis Indépendant Développement Durab...\n",
       "1    Elle prendra la forme d'une densification du r...\n",
       "2    Ce projet d'extension et de renforcement des r...\n",
       "3    32Annexe 6  Avis Indépendant Développement Dur...\n",
       "4    Elle prendra la forme d'une densification du r...\n",
       "..                                                 ...\n",
       "162  Le classement risque souverain s'étend de RCI ...\n",
       "163  30S oxouuy  I          Q[BIOOS UOTPIPSWII}UI |...\n",
       "164  9p mod Jo sINUeNd  Sose}solap sap souonboyy s...\n",
       "165  Elle prendra la forme d'une densification du r...\n",
       "166  Ce projet d'extension et de renforcement des r...\n",
       "\n",
       "[167 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scanextractor(path,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"C:/Users/marco/Desktop/Desktop/OECD/Codes/2014/Afrique/\"\n",
    "#\"CA II.3.1.1 BENIN.pdf\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
